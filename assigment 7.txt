Write a Python script that:
Use Genism to preprocess data from a sample text file, follow basic procedures like tokenization, stemming, lemmatization etc.
program:

import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import SnowballStemmer
import spacy
nlp = spacy.load('en_core_web_sm')
stemmer = SnowballStemmer('english')
text = "This is a sample text to demonstrate the use of Gensim for preprocessing."
tokens = simple_preprocess(text, deacc=True)
tokens = [token for token in tokens if token not in STOPWORDS]
stemmed_tokens = [stemmer.stem(token) for token in tokens]
doc = nlp(' '.join(stemmed_tokens))
lemmatized_tokens = [token.lemma_ for token in doc]
print("Original Text:", text)
print("Tokens:", tokens)
print("Stemmed Tokens:", stemmed_tokens)
print("Lemmatized Tokens:", lemmatized_tokens)

output:
Original Text: This is a sample text to demonstrate the use of Gensim for preprocessing.
Tokens: ['sample', 'text', 'demonstrate', 'use', 'gensim', 'preprocessing']
Stemmed Tokens: ['sampl', 'text', 'demonstr', 'use', 'gensim', 'preprocess']
Lemmatized Tokens: ['sampl', 'text', 'demonstr', 'use', 'gensim', 'preprocess']